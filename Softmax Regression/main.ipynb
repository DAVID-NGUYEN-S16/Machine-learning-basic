{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So please, remain that formular of Softmax regression\n",
    "\n",
    "> $a_i = \\frac{exp(z_i)}{\\sum_{j=1}^{C}exp(z_j)}, \\forall = 1, 2, ..., C$\n",
    "\n",
    "> And remain: $ z_i = w_i^Tx $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    e_z = np.exp(Z)\n",
    "    return e_z/ e_z.sum(axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because If you compute softmax in origin, So appear status overflow. Big influence to result. I solve by substract max values of per columns of Matrix Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(Z):\n",
    "    e_z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return e_z/e_z.sum(axis = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> About loss function, I used Cross entropy for compute loss between 2 distribution \n",
    "\n",
    "> $ H(p, q) = -\\sum_{i=1}^{C}p_ilog(q_i)$, In p is real, q is predict\n",
    "\n",
    "Above formula, I will apply for Sotfmax\n",
    "\n",
    "> $ J(W; x_i, y_i) = - \\sum_{j=1}^C y_{ji} log(a_{ji}) $\n",
    "\n",
    "> Gradient descent\n",
    ">\n",
    "> $ W = W + \\triangle J(W; x_i, y_i)  \\\\ W = W + \\lambda x_i(y_i - a_i)^T$\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# randomly generate data \n",
    "N = 2 # number of training sample \n",
    "d = 2 # data dimension \n",
    "C = 3 # number of classes \n",
    "\n",
    "X = np.random.randn(d, N)\n",
    "y = np.random.randint(0, 3, (N,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse \n",
    "def convert_labels(y, C = C):\n",
    "    \"\"\"\n",
    "    convert 1d label to a matrix label: each column of this \n",
    "    matrix coresponding to 1 element in y. In i-th column of Y, \n",
    "    only one non-zeros element located in the y[i]-th position, \n",
    "    and = 1 ex: y = [0, 2, 1, 0], and 3 classes then return\n",
    "\n",
    "            [[1, 0, 0, 1],\n",
    "             [0, 0, 1, 0],\n",
    "             [0, 1, 0, 0]]\n",
    "    \"\"\"\n",
    "    Y = sparse.coo_matrix((np.ones_like(y), \n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y \n",
    "\n",
    "Y = convert_labels(y, C)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.561252704992042e-10\n"
     ]
    }
   ],
   "source": [
    "# cost or loss function  \n",
    "def cost(X, Y, W):\n",
    "    A = softmax(W.T.dot(X))\n",
    "    return -np.sum(Y*np.log(A))\n",
    "\n",
    "W_init = np.random.randn(d, C)\n",
    "\n",
    "def grad(X, Y, W):\n",
    "    A = softmax((W.T.dot(X)))\n",
    "    E = A - Y\n",
    "    return X.dot(E.T)\n",
    "    \n",
    "def numerical_grad(X, Y, W, cost):\n",
    "    eps = 1e-6\n",
    "    g = np.zeros_like(W)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            W_p = W.copy()\n",
    "            W_n = W.copy()\n",
    "            W_p[i, j] += eps \n",
    "            W_n[i, j] -= eps\n",
    "            g[i,j] = (cost(X, Y, W_p) - cost(X, Y, W_n))/(2*eps)\n",
    "    return g \n",
    "\n",
    "g1 = grad(X, Y, W_init)\n",
    "g2 = numerical_grad(X, Y, W_init, cost)\n",
    "\n",
    "print(np.linalg.norm(g1 - g2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  softmax_regression(X, y, w_init, eta, tol = 1e-4, max_count =10000):\n",
    "    W = [w_init]\n",
    "    c = w_init.shape[1]\n",
    "    Y = convert_labels(y, c)\n",
    "    d, N = X.shape\n",
    "    count = 0\n",
    "    check_w_after = 20\n",
    "    while count < max_count:\n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[:, i].reshape(d, 1)\n",
    "            yi = Y[: , i].reshape(C, 1)\n",
    "            ai = softmax(np.dot(W[-1].T, xi))\n",
    "            w_new = W[-1] + eta*xi.dot((yi-ai).T)\n",
    "            \n",
    "            count+=1\n",
    "            # print(count)\n",
    "            if count%check_w_after == 0 and count != 0:\n",
    "                if np.linalg.norm(w_new - W[-check_w_after]) < tol:\n",
    "                    return W\n",
    "            W.append(w_new)\n",
    "    return W\n",
    "eta = .05\n",
    "d = X.shape[0]\n",
    "W_init = np.random.randn(d, C)\n",
    "W = softmax_regression(X, y, W_init, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, x):\n",
    "    A = softmax_stable(w.T.dot(X))\n",
    "    print(A)\n",
    "    return np.argmax(A, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.95771125e-01 5.00245053e-03]\n",
      " [2.42694990e-06 9.76379980e-01]\n",
      " [4.22644777e-03 1.86175693e-02]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(W[-1], X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
